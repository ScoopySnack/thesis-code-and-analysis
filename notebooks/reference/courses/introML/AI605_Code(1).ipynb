{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " we are going to use the Wine dataset to illustrate different \n",
    "performance metrics for our algorithms.\n",
    "\n",
    "You can find more info about it here: https://archive.ics.uci.edu/ml/datasets/wine\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# To randomly split the data into train/test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "We will compare the performance of all of the classifiers we have seen\n",
    "so far in previous weeks, so we import them all\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB  # I will just use one of the Naive Bayes we know about\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We are not going to do any optimisation or tuning on them, so let's just use them\n",
    "with their default options. We will save the constructor functions for all of them \n",
    "in a list (array) called \"classifiers\", and then call those default constructor\n",
    "functions and save their instantiation (objects) in a list called \"models\"\n",
    "Note: You can have seen many different parameters\n",
    "for all of these classifiers, so feel free to tune them by adding some parameters to\n",
    "these constructors.\n",
    "\"\"\"\n",
    "classifiers = [LogisticRegression, KNeighborsClassifier, MultinomialNB, \n",
    "               DecisionTreeClassifier, RandomForestClassifier]\n",
    "\n",
    "models = [classifier() for classifier in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will download the dataset, we will note the input features and the target\n",
    "that we are going to classify (in this case we have 3 classes)\n",
    "\"\"\"\n",
    "wine_data = load_wine(return_X_y=False, as_frame=True)\n",
    "wine_data = wine_data.frame # We just take the Pandas DataFrame from the data\n",
    "input_features = list(wine_data.columns[:-1]) # All of the columns, apart from the last one, are input features - the last column is the target feature\n",
    "wine_data  # We can display the data like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will split the data into a train dataset (which \n",
    "we will use to train/fit our models) and a test dataset (which we will use to test \n",
    "them -> AND to evaluate their performance a little bit better than with just the accuracy)\n",
    "\"\"\"\n",
    "PERCENTAGE_SAMPLES_USED_FOR_TESTING = 0.4\n",
    "\n",
    "train, test = train_test_split(wine_data, test_size=PERCENTAGE_SAMPLES_USED_FOR_TESTING)\n",
    "print('We are using', len(train),'samples for training the', len(models),'models and',len(test),'samples for testing them later.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can train all of our models in a loop\n",
    "\"\"\"\n",
    "for m in models:\n",
    "    m.fit(train[input_features], train['target'])\n",
    "    \n",
    "\"\"\"\n",
    "And once they are fit (trained) we can then use them to make predictions/estimations for\n",
    "the test data samples, and compare those predictions/estimations with the \"real\" target\n",
    "values in the test data.\n",
    "Here I will use the score function, that calculates the accuracy (from 0 to 1 where 0 is\n",
    "0% accurate and 1 is 100% accurate - i.e. it guessed correctly all of the test samples).\n",
    "\n",
    "\n",
    "Note: You might get some warning for some algorithm, because we are not tuning them and\n",
    "the default parameters not always work - feel free to tune it or let's just go with\n",
    "whatever accuracy.\n",
    "\"\"\"\n",
    "for m in models:\n",
    "    score = m.score(test[input_features], test['target'])\n",
    "    print(str(m), \": \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's look at their balanced accuracies, to understand what they are doing well and wrong...\n",
    "\"\"\"\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "for m in models:\n",
    "    predictions = m.predict(test[input_features])\n",
    "    print()\n",
    "    print(\"Balanced Accuracy Scores for\", str(m), \":\")\n",
    "    balanced_accuracies = balanced_accuracy_score(test['target'], predictions)\n",
    "    print(balanced_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Those accuracies don't tell us much about what classes perform better than others...\n",
    "So let's look at the Confusion Matrices\n",
    "\"\"\"\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "for m in models:\n",
    "    predictions = m.predict(test[input_features])\n",
    "    # cm will be a list of lists (matrix), with the values of the confusion matrix...\n",
    "    cm = confusion_matrix(test['target'], predictions, labels=m.classes_)\n",
    "    print()\n",
    "    print(\"Confusion matrix for\", str(m), \":\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ... however it's better to just plot it out to visualise it.\n",
    "# In one function we're doing the predictions in the test set and plotting them:\n",
    "\n",
    "# We are doing them one by one in different cells:\n",
    "print(\"Confusion Matrix for \", str(models[0]))\n",
    "plot_confusion_matrix(models[0], \n",
    "                      X=test[input_features], \n",
    "                      y_true=test['target'], \n",
    "                      labels=models[0].classes_,\n",
    "                      cmap='Blues') # we changed the colormap to something less psychodelic than the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix for \", str(models[1]))\n",
    "plot_confusion_matrix(models[1], \n",
    "                      X=test[input_features], \n",
    "                      y_true=test['target'], \n",
    "                      labels=models[1].classes_,\n",
    "                      cmap='Blues') #We changed the colormap to something less psychodelic than the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix for \", str(models[2]))\n",
    "plot_confusion_matrix(models[2], \n",
    "                      X=test[input_features], \n",
    "                      y_true=test['target'], \n",
    "                      labels=models[2].classes_,\n",
    "                      cmap='Blues') # We  changed the colormap to something less psychodelic than the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix for \", str(models[3]))\n",
    "plot_confusion_matrix(models[3], \n",
    "                      X=test[input_features], \n",
    "                      y_true=test['target'], \n",
    "                      labels=models[3].classes_,\n",
    "                      cmap='Blues') # We did the same, we changed the colormap to something less psychodelic than the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix for \", str(models[4]))\n",
    "plot_confusion_matrix(models[4], \n",
    "                      X=test[input_features], \n",
    "                      y_true=test['target'], \n",
    "                      labels=models[4].classes_,\n",
    "                      cmap='Blues') # the colormap to something less psychodelic than the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The Confusion Matrices above whose diagonals are darker mean that they are better models. You can also see in them which classes your models are predicting better than others. \n",
    "\n",
    "The data is randomised so maybe this is not the case for you but in some of the runs it seems like KNN is struggling with Wine Type 2 (class \"2\"). And this is what mainly causes its low performance. RandomForest on the other hand is always predicting class \"2\" correctly, and its mistakes come from confusing classes \"0\" and \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's calculate again the confusion matrices, but this time normalised:\n",
    "\n",
    "Then, the diagonal of the confusion matrix are the accuracies for each class!!\n",
    "So we can calculate the Balance Accuracy in this way, which is relatively simple:\n",
    "  * First calculate each class' individual accuracy\n",
    "  * Then average those individual class accuracies\n",
    "\n",
    "\"\"\"\n",
    "for m in models:\n",
    "    predictions = m.predict(test[input_features])\n",
    "    # cm will be a list of lists (matrix), with the values of the confusion matrix...\n",
    "    cm = confusion_matrix(test['target'], predictions, labels=m.classes_, normalize='true')\n",
    "    print('\\n\\n')\n",
    "    print(\"Confusion matrix for\", str(m), \":\")\n",
    "    print(cm)\n",
    "    print(\"  * Accuracies per class: \")\n",
    "    print('    ', list(zip(['Class 0', 'Class 1', 'Class 2'], [round(x, 2) for x in cm.diagonal()])))\n",
    "    print(\"  * Balanced Accuracy for \", str(m), \":\")\n",
    "    print('    ', round(sum(cm.diagonal())/len(cm.diagonal()), 4))\n",
    "    print('(this should be the same balanced score value that we calculated above, using the balanced_accuracy_score function)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to remember the slides of this session, many classification performance metrics such as precision, recall and f1-score are derived from the Confusion Matrix by counting right and wrong predictions in different ways (false positives, true negatives, etc.).\n",
    "\n",
    "A very interesting function in sklearn that you can use to get a glance of there metrics, per class, is classification_report. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    predictions = m.predict(test[input_features])\n",
    "    print('***** Classification report for ', str(m), '***** ')\n",
    "    print(classification_report(test['target'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there how poorly KNN performs in Class 2, across all of the metrics\n",
    "\n",
    "If you remember from the slides, the precision, recall and f1-score metrics were designed for binary classifiers, so... why does it work in this case with 3 classes?\n",
    "\n",
    "Well, what sklearn is doing when calculating them per class is \"transforming the problem into a binary classifier\" by calculating the metrics as: the current class vs all other classes. In this way the problem becomes a binary classifier, whatever the number of classes we have.\n",
    "\n",
    "We could have also calculated each of these metrics separately. In fact, we could have calculated many other metrics, all of them available in the sklearn.metrics package (check docs here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). I will leave that to you...\n",
    "\n",
    "### try to do the following:\n",
    "Can you calculate the precision, recall and f1-scores separately using their corresponding function in the metrics package?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for m in models:\n",
    "    # We will need the proabilities, so the X axis of the ROC curve can be calculated\n",
    "    # (i.e. for different thresholds, remember?)\n",
    "    predictions = m.predict_proba(test[input_features])\n",
    "    print('***** ROC Area Under the Curve for', str(m), '***** ')\n",
    "    print(roc_auc_score(y_true=list(test['target']), \n",
    "                        y_score=predictions, \n",
    "                        multi_class='ovr')\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So RandomForest is not just the classifier with the highest accuracy, but also the one that better differentiates the classes between them!\n",
    "And remember, ROC works for binary classifiers, so in this multi-class problem we are specifying the OVR strategy: one-vs-rest, that is one class vs the rest of classes, and then average out the AUC scores for all 3 classes.\n",
    "\n",
    "We could plot those ROC curves so we can visualise the five performances if our classification problem was binary, just by using the code given by sklearn here:\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "In short: We'd have to apply the one-vs-rest strategy into the classifier (there is a special type of meta-classifier that applies this strategy on top of any other classifier), and then we'd manually plot the data returned by the auc function in the metrics package.\n",
    "\n",
    "If you ever have a binary classification problem, just do:\n",
    "\n",
    "```\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(models[0], test[input_features], test['target'])\n",
    "```\n",
    "\n",
    "However if you run that code here it'll complain that your classifiers are not binary... :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning exercise:\n",
    "\n",
    "Finally, note that we also have all of the [Regression success metrics in the `sklearn.metrics` package](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "you can experiment further, if you want to follow these steps: \n",
    "1. load the [diabetes data, which is a regression problem](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes),\n",
    "2. fit and predict a couple of regression models to that data, for example [linear regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression) and [ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) and \n",
    "3. apply some of the popular metrics such as MAE, MSE, RMSE and R^2, available in `sklearn.metrics`. You just need to apply them the same way we did with the more complex classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
