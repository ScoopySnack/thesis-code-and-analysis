{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. We load some data\n",
    "\n",
    "* If you are unfamiliar with the Iris Flower dataset, check it here: https://en.wikipedia.org/wiki/Iris_flower_data_set . In Machine Learning it is so popular that it becomes already pre-loaded in SciKit Learn, that's why I can import it using `from sklearn.datasets import load_iris`\n",
    "\n",
    "* Once the data is loaded in \"iris\", we discard one of the three classes, to make a binary classification (so we can better explain and plot Logistic Regression's inner workings. We display the number of records we will work with after we discard one class (Feel free to change the class discarded: it can be 0, 1 or 2).\n",
    "* After we have just 2 classes, we split the data in training and testing. Training will be used to build the Logistic Regression model, and testing to test the model and see how accurate it is. Feel free to change the % of data used for testing - it is set to 0.8 (80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we download some data\n",
    "iris_data = load_iris(return_X_y=True, as_frame=True)\n",
    "iris = iris_data[0]\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "iris.columns = features\n",
    "iris['type'] = iris_data[1] # We add the target (class) column to the features dataframe\n",
    "del iris_data # using del we delete some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we take just 2 classes to perform some binary classification\n",
    "# We have 3 classes [0, 1 and 2], so just choose 2 of them to classify from.\n",
    "# We are discarding class 0 (therefore classifying 1 vs 2), but feel free to change it to:\n",
    "# CLASS_DISCARDED = 1 (we'd classify between 0 and 2)\n",
    "# CLASS_DISCARDED = 2 (we'd classify between 0 and 1)\n",
    "CLASS_DISCARDED = 0\n",
    "iris_binary = iris[iris['type'] != CLASS_DISCARDED]\n",
    "\n",
    "print('We will work with',len(iris_binary),'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTAGE_SAMPLES_USED_FOR_TESTING = 0.8\n",
    "\n",
    "train, test = train_test_split(iris_binary, test_size=PERCENTAGE_SAMPLES_USED_FOR_TESTING)\n",
    "print('Using', len(train),'samples for training the model and',len(test),'samples for testing it later.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. We fit and test our Logistic Regression classifier\n",
    "\n",
    "* You can check all the parameters that Scikit-Learn allows to tune our Logistic Regression model: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "* Once we create the classifier, we can fit it (build the \"best fit\" sigmoid function as we saw in the slides), using the training data we separated in the previous section\n",
    "* You can change the number of max iterations for the optimisation process to find the best fit sigmoid (usually this is a maximum likelihood process, but can be some other too)\n",
    "* Once the model is fit, we display the intercept and the coefficients of the 4 features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "* We also print the expit function that can be used to transform the odds into class probabilities (i.e. to move the data from the logit chart to the sigmoid function chart)\n",
    "* Then we use the fit model to predict the data we left aside for testing (test data), and we print the predictions.\n",
    "* Those predictions of one class or another are based on whether the probability we obtained in the expit function is over 0.5 or not. We can change that threshold to improve the accuracy of our model with future data.\n",
    "* For that, we plot the data in scatter plots, differentiating the two classes, and ask you to select a better threshold than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we say to Scikit-Learn the type of classification model we want to make\n",
    "\n",
    "# Change the number of iterations that the Maximum Likehood Estimator has to converge. Lower numbers should affect\n",
    "# performance\n",
    "MAX_ITERATIONS_MLE = 10\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    random_state=0,  # Shuffling the data\n",
    "    solver='liblinear',  # No need to use Gradient Descent or anything fancy to find the MLE for such small dataset\n",
    "    penalty='l2',  # We apply L2 regularisation (same regularisation we apply in regression models)\n",
    "    n_jobs=1,  # Use one processor in this machine for the MLE process\n",
    "    max_iter=10,  # Max number of iterations for the MLE process\n",
    ")\n",
    "# Note: MLE is the Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we fit our model (the sigmoid function we saw in the slides) using the training data.\n",
    "# The function classifier.fit(X, y) expects:\n",
    "# 1. A matrix X (or array of arrays) containing one row per data record, each containing a value for each feature: in our case 4 features.\n",
    "# 2. An array with the class values that is expected for each data record. \n",
    "classifier.fit(train[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']], train['type'])\n",
    "\n",
    "# Number of iterations until the MLE process converged.\n",
    "print('The classifier was correctly fit given the',len(train), 'records of training data.')\n",
    "print('* The classifier optimisation MLE process took', classifier.n_iter_[0], 'iterations to converge')\n",
    "print('* The intercept of the Logistic Regression is:',classifier.intercept_[0])\n",
    "print('* And the coefficients for each feature are:',classifier.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a small utility to print the sigmoid function for you guys\n",
    "def print_sigmoid_function(classifier):\n",
    "    formula = str(round(classifier.intercept_[0], 2))\n",
    "    for coef, column in zip(classifier.coef_[0], features):\n",
    "        formula += ' + (' + str(round(coef, 2)) + '*' + column +')'\n",
    "    print('The expit function, used to transform the odds into class probabilities, is: type=1/(1+e^(-F))')\n",
    "    print('Where F is: F=',formula)\n",
    "\n",
    "print_sigmoid_function(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(test[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The classifier\\'s predictions over the testing data are:\\n',list(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('And the real testing data we were expecting is:\\n',list(test['type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is behaving pretty good already:\n",
    "right = 0\n",
    "for real, pred in zip(test['type'], preds):  # With ZIP we join two arrays together, element by element, like a zip\n",
    "    if real == pred:\n",
    "        right += 1\n",
    "print('Correctly guessed:', right, '/',len(preds),'(', round(100*right/len(preds), 2), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model's coefficients and intercept, we can calculate ourselves the probability\n",
    "# for each test record to belong to one class or another.\n",
    "types = []\n",
    "\n",
    "test_values = test[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "\n",
    "for r in test_values:\n",
    "    t = classifier.intercept_[0] + sum(val*coef for val, coef in zip(r, classifier.coef_[0]))\n",
    "    p = 1/(1+math.exp(-t))\n",
    "    types.append(round(p, 4))\n",
    "\n",
    "prob_reals = list(zip(types, test['type']))\n",
    "\n",
    "\n",
    "\n",
    "# The previous calculation we did using the sigmoid function (aka expit function to get the actual class\n",
    "# probabilities) is also implemented in sKlearn using:\n",
    "NUM_PROB_PREDS_TO_PRINT = 10\n",
    "\n",
    "print('''In these tuples, the first value is the probability of belonging to one class, and the second \n",
    "is the real class we expected. Low probabilities (up to 0.5) are predicted to one class, while high \n",
    "probabilities are predicted to another class.\\n\\n''')\n",
    "\n",
    "print('We print the first', NUM_PROB_PREDS_TO_PRINT,'class probabilities we calculated along with the real class we were expecting:')\n",
    "print(prob_reals[:NUM_PROB_PREDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('When sklearn calculates the probabilities, it does for the two classes: The 2 values in each prediction sum 1. The second value is what we calculated above:')\n",
    "pred_probs = classifier.predict_proba(test[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "print('Printing the first',NUM_PROB_PREDS_TO_PRINT,'values:')\n",
    "print(pred_probs[:NUM_PROB_PREDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I defined a function that plots the 2 classes selected, and the threshold by which Sklearn decides to which\n",
    "# class each tested data belongs to...\n",
    "# Below you will see that 0.5 is not necessarily the best threshold, and we can set some other to achieve better\n",
    "# accuracy.\n",
    "def plot_prediction_probs_with_real_class(prob_reals, PROBABILITY_THRESHOLD=None):\n",
    "    ones = []\n",
    "    twos = []\n",
    "    first_class = None\n",
    "    second_class = None\n",
    "    for prob, real in prob_reals:\n",
    "        if not first_class:\n",
    "            first_class = real\n",
    "        if not second_class and real != first_class:\n",
    "            second_class = real\n",
    "        if real == first_class:\n",
    "            ones.append(prob)\n",
    "        else:\n",
    "            twos.append(prob)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    ax1.scatter(x=range(len(ones)), y=ones, c='red', marker='s', label='Flower type '+str(first_class))\n",
    "    ax1.scatter(x=range(len(twos)), y=twos, c='blue', marker='o', label='Flower type '+str(second_class))\n",
    "    items_in_x_axis = max(len(ones), len(twos))\n",
    "    ax1.plot(range(items_in_x_axis), \n",
    "             [0.5]*items_in_x_axis, \n",
    "             color='grey', \n",
    "             linestyle='--',\n",
    "             label='Sklearn threshold'\n",
    "            )\n",
    "    if PROBABILITY_THRESHOLD:\n",
    "        ax1.plot(range(items_in_x_axis), \n",
    "                 [PROBABILITY_THRESHOLD]*items_in_x_axis, \n",
    "                 color='black', \n",
    "                 linewidth=1.5,\n",
    "                 linestyle='--',\n",
    "                 label='Our new threshold'\n",
    "                )\n",
    "\n",
    "    plt.title('We can see how our classifier assigns different probabilites to each class, differentiating them:')\n",
    "    plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5));\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "print('''The most towards the bottom and top all of the probabilities for the records are, the more sure the \n",
    "classifier is that it is classifying that record correctly!\\n\\n''')\n",
    "\n",
    "print('Correctly guessed:', right, '/',len(preds),'(', round(100*right/len(preds), 2), '%)')\n",
    "plot_prediction_probs_with_real_class(prob_reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A threshold of 0.5 is what sklearn is using. Based on the chart below, can you find a\n",
    "# better threshold for our data? Input it there and see what accuracy we get.\n",
    "PROBABILITY_THRESHOLD = 0.4\n",
    "\n",
    "classes = classifier.classes_\n",
    "right_predictions = 0\n",
    "for prob, real in prob_reals:\n",
    "    if prob > PROBABILITY_THRESHOLD:\n",
    "        prediction=classes[-1]\n",
    "    else:\n",
    "        prediction=classes[0]\n",
    "    if prediction==real:\n",
    "        right_predictions+=1\n",
    "print('Correctly guessed with new threshold:', \n",
    "      right_predictions, '/',len(preds),'(', round(100*right_predictions/len(preds), 2), '%)')\n",
    "\n",
    "\n",
    "plot_prediction_probs_with_real_class(prob_reals, PROBABILITY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. We do some basic feature selection\n",
    "\n",
    "* We do this to see which features are really contributing to guess correctly the flower type class.\n",
    "\n",
    "* We can use a chi-squared test to see the z-scores and p-values of each feature in relation to the flower type class (basically using the same data that we used for training the model)\n",
    "* After checking the p-values and z-scores in the cells below, with their comments, we can build another Logistic Regression classifier but using just some of the features, not all of them. If we removed features that were not helping much in the result, the accuracy of this new classifier shouldn't be much worse (maybe even better) than the previous one, only that we are using much less data to build it and therefore it is lighter and faster to build (if our dataset had millions of rows, the speed and computational space factor would be an issue). \n",
    "* You can coment out or remove features below and rebuild again the Logistic Regression classifier re-running the cells, and checking how the accuracy is affected.\n",
    "* Like before, we added a probability threshold that you can tune to maximise the accuracy after checking the results in a scatter chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do some statistical feature selection to see which features explain the target (flower type)\n",
    "# and which ones don't. We use the Chi-squared test for that:\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "scores, pvalues = chi2(train[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']], train['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The p-value for sepal_width is very high, indicating that it is the most independent from the class. \n",
    "# In other words, it seems like sepal_width is the feature adding the least information to differentiate \n",
    "# classes 1 and 2.\n",
    "list(zip(features, pvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The samples are being randomised, and in some cases sepal_length is a bit worse and actually giving less \n",
    "# info than sepal_width, so check the parameters being printed by the code to follow up... but keep that in mind\n",
    "\n",
    "# We can also see their z-scores, the sepal_width parameter is not even 1 standard dev away from the average of the \n",
    "# class distribution. Usually, variables with over |2| z-score are the relevant ones, or at least |1|.\n",
    "list(zip(features, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which features to use to see which ones still keep a good performance so we can simplify the model:\n",
    "# We removed sepal_width for you. Can we remove some more for the classes you're classifying?\n",
    "features = [\n",
    "    'sepal_length', \n",
    "#     'sepal_width', \n",
    "    'petal_length', \n",
    "    'petal_width'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = LogisticRegression(\n",
    "    random_state=0, penalty='l2'\n",
    ")\n",
    "classifier2.fit(train[features], train['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_preds = classifier2.predict(test[features])\n",
    "right_predictions = 0\n",
    "for pred, real in zip(default_preds, test['type']):\n",
    "    if pred==real:\n",
    "        right_predictions+=1\n",
    "print('Correctly guessed with default threshold of 0.5 :', right_predictions, '/',len(default_preds),'(', round(100*right_predictions/len(default_preds), 2), '%)')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we predict again but getting the probabilities so we can plot the predictions\n",
    "preds = classifier2.predict_proba(test[features])\n",
    "\n",
    "PROBABILITY_THRESHOLD_2 = 0.38\n",
    "\n",
    "prob_reals_2 = []\n",
    "classes = classifier.classes_\n",
    "right_predictions = 0\n",
    "for probs, real in zip(preds, test['type']):\n",
    "    prob_reals_2.append((probs[1], real))\n",
    "    if probs[1] > PROBABILITY_THRESHOLD_2:\n",
    "        prediction=classes[-1]\n",
    "    else:\n",
    "        prediction=classes[0]\n",
    "    if prediction==real:\n",
    "        right_predictions+=1\n",
    "print('Correctly guessed with thresold of ', PROBABILITY_THRESHOLD_2,':', right_predictions, '/',len(preds),'(', round(100*right_predictions/len(preds), 2), '%)')\n",
    "\n",
    "plot_prediction_probs_with_real_class(prob_reals_2, PROBABILITY_THRESHOLD_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you want to do some basic Linear Regression too?\n",
    "\n",
    "Of course SciKit Learn also has the classic linear regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Seeing how we have trained the Logistic Regression classifier above, could you do the same for Linear Regression just checking the documentation above?\n",
    "\n",
    "Note that linear regression is a *true* regression method, so it will return a real value when calling its \"predict\" function. However you can round that value to the nearest class (0, 1 or 2) if you wanted, so you can use it as a classifier, and also measure its accuracy as we did above (number of correctly predicted test samples over size of the test dataset). You can call the same methods that we used for Logistic Regression: `fit` to fit the regression line, and `predict` to test the test dataset and obtain the predictions for the test samples.\n",
    "\n",
    "Using Linear Regression as a *benchmark test* is a good idea because many times it's considered one of the most quick and basic methods. Also in many occassions, if the data is good enough, a simple Linear Regression might already give us good enough results and therefore we do not need to do anything more complicated.\n",
    "\n",
    "If you feel very inspired, feel free to use the matplotlib library to plot some other stuff you might think is relevant to show the linear regression model, like the correctly and wrongly classified samples using your fitted linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(\n",
    "    n_jobs=1,  # Use one processor in this machine for the MLE process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
